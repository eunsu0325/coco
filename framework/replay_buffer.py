# framework/replay_buffer.py - ìˆ˜ì •ëœ ë²„ì „ (Faiss ì˜¤ë¥˜ í•´ê²°)
"""
COCONUT Intelligent Replay Buffer with Fixed Faiss Reconstruction

ğŸ”§ CRITICAL FIX:
- Fixed Faiss reconstruct error in _cull() method
- Uses stored embeddings instead of Faiss reconstruction
- Maintains all original functionality with better reliability

DESIGN PHILOSOPHY:
- Diversity-based sample selection using Faiss similarity search
- Original images stored for exact replay (no degradation)
- Metadata management for rich context information
- Efficient culling strategy to maintain buffer size
- ğŸ†• Proper batch size support with replacement sampling
"""

import os
import pickle
from pathlib import Path

import faiss
import numpy as np
import torch

class CoconutReplayBuffer:
    def __init__(self, config, storage_dir: Path, feature_dimension: int = 2048):
        """
        COCONUTì˜ ì§€ëŠ¥í˜• ê¸°ì–µ ì¥ì¹˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        DESIGN RATIONALE:
        - Faiss index for efficient high-dimensional similarity search
        - Diversity threshold prevents redundant samples
        - Original image storage ensures perfect replay quality
        - ğŸ†• Support for replacement sampling to guarantee batch sizes
        - ğŸ”§ Fixed Faiss reconstruction issue in culling
        """
        self.config = config
        self.storage_dir = storage_dir
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        self.buffer_size = self.config.max_buffer_size
        self.similarity_threshold = self.config.similarity_threshold
        self.feature_dimension = feature_dimension

        # Faiss: ê³ ì°¨ì› ë²¡í„°ì˜ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤
        self.image_storage = []  # ì›ë³¸ ì´ë¯¸ì§€ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        self.faiss_index = None  # ë‹¤ì–‘ì„± ì¸¡ì •ìš©
        
        # ğŸ”§ FIX: ì„ë² ë”©ì„ ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ reconstruction ë¬¸ì œ í•´ê²°
        self.stored_embeddings = []  # ì„ë² ë”©ì„ ì§ì ‘ ì €ì¥

        # ë²„í¼ì— ì €ì¥ëœ ë°ì´í„°ì˜ ë©”íƒ€ì •ë³´ ê´€ë¦¬
        self.metadata = {}

        # íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ëª¨ë¸ ì°¸ì¡°
        self.feature_extractor = None

        # ì‹œìŠ¤í…œ ì¬ì‹œì‘ì„ ìœ„í•œ ìƒíƒœ íŒŒì¼ ê²½ë¡œ
        self.state_file = self.storage_dir / 'buffer_state.pkl'
        self._load_state() # ì‹œì‘ ì‹œ ìë™ìœ¼ë¡œ ìƒíƒœ ë³µì› ì‹œë„

        print(f"[Buffer] Replay Buffer initialized. Max size: {self.buffer_size}, Sim threshold: {self.similarity_threshold}")
        print(f"[Buffer] Current buffer size: {self.faiss_index.ntotal if self.faiss_index else 0}")

    def set_feature_extractor(self, model):
        """íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ëª¨ë¸ ì„¤ì •"""
        self.feature_extractor = model

    def _initialize_faiss(self):
        """Faiss ì¸ë±ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        index = faiss.IndexFlatIP(self.feature_dimension)
        self.faiss_index = faiss.IndexIDMap(index)
        print(f"[Buffer] Faiss index initialized with dimension {self.feature_dimension}.")

    def _extract_feature_for_diversity(self, image):
        """ë‹¤ì–‘ì„± ì¸¡ì •ì„ ìœ„í•œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
        if self.feature_extractor is None:
            raise ValueError("Feature extractor not set. Call set_feature_extractor() first.")
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°)
        if len(image.shape) == 3:
            image = image.unsqueeze(0)
        
        # CCNetì˜ getFeatureCode ë©”ì„œë“œ ì‚¬ìš©
        with torch.no_grad():
            features = self.feature_extractor.getFeatureCode(image)
        
        return features

    def add(self, image: torch.Tensor, user_id: int):
        """
        ìƒˆë¡œìš´ ê²½í—˜(ì´ë¯¸ì§€)ì„ ë²„í¼ì— ì¶”ê°€í• ì§€ ê²°ì •í•˜ê³  ì¶”ê°€í•©ë‹ˆë‹¤.
        
        DIVERSITY-BASED ADDITION:
        1. Extract feature embedding from new image
        2. Calculate similarity with existing samples
        3. Add only if sufficiently diverse (below threshold)
        """
        # 1. ë‹¤ì–‘ì„± ì¸¡ì •ì„ ìœ„í•œ ì„ë² ë”© ì¶”ì¶œ
        with torch.no_grad():
            embedding = self._extract_feature_for_diversity(image)
            embedding_np = embedding.cpu().numpy().astype('float32')
            faiss.normalize_L2(embedding_np)

        if self.faiss_index is None:
            self._initialize_faiss()

        # 2. ë‹¤ì–‘ì„± í™•ì¸
        if self.faiss_index.ntotal == 0:
            max_similarity = 0.0
        else:
            distances, _ = self.faiss_index.search(embedding_np, k=1)
            max_similarity = distances[0][0]

        # 3. ìƒˆë¡œìš´ ê²½í—˜ì´ë©´ ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
        if max_similarity < self.similarity_threshold:
            if len(self.image_storage) >= self.buffer_size:
                self._cull()  # ì˜¤ë˜ëœ ì´ë¯¸ì§€ ì œê±°
            
            unique_id = len(self.image_storage)
            self.image_storage.append({
                'image': image.cpu().clone(),  # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
                'user_id': user_id,
                'id': unique_id
            })
            
            # ğŸ”§ FIX: ì„ë² ë”©ë„ ë³„ë„ë¡œ ì €ì¥
            self.stored_embeddings.append(embedding_np.copy())
            
            # ë‹¤ì–‘ì„± ì¸¡ì •ìš© faiss ì¸ë±ìŠ¤ë„ ì—…ë°ì´íŠ¸
            self.faiss_index.add_with_ids(embedding_np, np.array([unique_id]))
            self.metadata[unique_id] = {'user_id': user_id}
            
            print(f"[Buffer] Added new diverse sample (ID: {unique_id}, User: {user_id}). "
                  f"Buffer size: {len(self.image_storage)}/{self.buffer_size}")
        else:
            print(f"[Buffer] Sample too similar (similarity: {max_similarity:.4f}). Skipped.")
            
    def _cull(self):
        """
        ğŸ”§ FIXED: ë²„í¼ ë‚´ì—ì„œ ê°€ì¥ ì¤‘ë³µë˜ëŠ” ë°ì´í„°ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        
        Faiss reconstruction ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì €ì¥ëœ ì„ë² ë”©ì„ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        if self.faiss_index.ntotal < 2:
            return

        print(f"[Buffer] Buffer is full ({self.faiss_index.ntotal}). Culling least diverse sample...")
        
        # ğŸ”§ FIX: Faiss reconstruction ëŒ€ì‹  ì €ì¥ëœ ì„ë² ë”© ì‚¬ìš©
        if len(self.stored_embeddings) == 0:
            print("[Buffer] Warning: No stored embeddings available for culling")
            return
        
        # ì €ì¥ëœ ì„ë² ë”©ë“¤ì„ NumPy ë°°ì—´ë¡œ ë³€í™˜
        all_vectors = np.vstack(self.stored_embeddings)
        
        k = min(self.faiss_index.ntotal, 50) 
        similarities, _ = self.faiss_index.search(all_vectors, k=k)
        
        # ê° ë²¡í„°ì˜ ìœ ì‚¬ë„ ì´í•© ê³„ì‚° (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„(1.0)ëŠ” ì œì™¸)
        diversity_scores = similarities.sum(axis=1) - 1.0
        
        cull_idx_in_storage = np.argmax(diversity_scores)
        
        # ì‹¤ì œ unique_id ì°¾ê¸°
        cull_unique_id = self.image_storage[cull_idx_in_storage]['id']

        # Faiss ì¸ë±ìŠ¤ì—ì„œ ì œê±°
        try:
            self.faiss_index.remove_ids(np.array([cull_unique_id]))
        except Exception as e:
            print(f"[Buffer] Warning: Faiss removal failed: {e}")
            # Faiss ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶•
            self._rebuild_faiss_index_after_removal(cull_idx_in_storage)
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°
        if cull_unique_id in self.metadata:
            del self.metadata[cull_unique_id]
        
        # ì´ë¯¸ì§€ ì €ì¥ì†Œì—ì„œë„ ì œê±°
        del self.image_storage[cull_idx_in_storage]
        
        # ğŸ”§ FIX: ì €ì¥ëœ ì„ë² ë”©ì—ì„œë„ ì œê±°
        del self.stored_embeddings[cull_idx_in_storage]
        
        print(f"[Buffer] Removed sample with unique ID: {cull_unique_id}")

    def _rebuild_faiss_index_after_removal(self, removed_idx):
        """
        ğŸ”§ FIX: Faiss ì¸ë±ìŠ¤ ì¬êµ¬ì¶• (ì œê±° ì‹¤íŒ¨ ì‹œ ë°±ì—… ë°©ë²•)
        """
        print("[Buffer] Rebuilding Faiss index after removal failure...")
        
        # ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„±
        self._initialize_faiss()
        
        # ì œê±°ë  í•­ëª©ì„ ì œì™¸í•˜ê³  ë‹¤ì‹œ ì¶”ê°€
        for i, (item, embedding) in enumerate(zip(self.image_storage, self.stored_embeddings)):
            if i != removed_idx:  # ì œê±°ë  í•­ëª© ì œì™¸
                self.faiss_index.add_with_ids(
                    embedding.reshape(1, -1), 
                    np.array([item['id']])
                )

    def sample(self, batch_size: int):
        """
        ğŸ”§ ìˆ˜ì •ëœ ê¸°ë³¸ ìƒ˜í”Œë§ - ì´ì œ sample_with_replacement í˜¸ì¶œ
        
        KEY CHANGE: ê¸°ì¡´ ì œí•œì  ìƒ˜í”Œë§ ëŒ€ì‹  ë³µì› ì¶”ì¶œ ì‚¬ìš©
        """
        return self.sample_with_replacement(batch_size)

    def sample_with_replacement(self, batch_size: int):
        """
        ğŸ”¥ ë³µì› ì¶”ì¶œë¡œ ì¶©ë¶„í•œ ìƒ˜í”Œ í™•ë³´ - W2ML í•µì‹¬ ìˆ˜ì •ì‚¬í•­
        
        KEY FIX: ë²„í¼ í¬ê¸°ì— ê´€ê³„ì—†ì´ ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
        
        BENEFITS:
        - í•­ìƒ ì„¤ì •ëœ batch_size ë‹¬ì„±
        - W2MLì´ ì¶©ë¶„í•œ ëŒ€ì¡° ìŒìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥
        - ì§„ì •í•œ ë°°ì¹˜ í¬ê¸° í™œìš©
        
        Args:
            batch_size: ìš”ì²­í•  ìƒ˜í”Œ ê°œìˆ˜
            
        Returns:
            images: ë³µì› ì¶”ì¶œëœ ì´ë¯¸ì§€ë“¤
            labels: í•´ë‹¹ ì‚¬ìš©ì IDë“¤
        """
        if len(self.image_storage) == 0:
            print("[Buffer] âš ï¸ Empty buffer - returning empty lists")
            return [], []
        
        if len(self.image_storage) == 1:
            # ë²„í¼ì— 1ê°œë§Œ ìˆìœ¼ë©´ ê·¸ê²ƒë§Œ ë°˜ë³µ
            item = self.image_storage[0]
            images = [item['image'].clone() for _ in range(batch_size)]
            labels = [item['user_id'] for _ in range(batch_size)]
            
            print(f"[Buffer] ğŸ”„ Single sample replication: {batch_size} copies of User {item['user_id']}")
            return images, labels
        
        # ë³µì› ì¶”ì¶œë¡œ í•„ìš”í•œ ê°œìˆ˜ë§Œí¼ ìƒ˜í”Œë§
        sampled_indices = np.random.choice(
            len(self.image_storage), 
            size=batch_size, 
            replace=True  # ğŸ”¥ í•µì‹¬: ë³µì› ì¶”ì¶œ í—ˆìš©
        )
        
        images = []
        labels = []
        sampled_users = []
        
        for idx in sampled_indices:
            item = self.image_storage[idx]
            images.append(item['image'].clone())  # ë³µì‚¬ë³¸ ìƒì„± (ì¤‘ìš”!)
            labels.append(item['user_id'])
            sampled_users.append(item['user_id'])
        
        # ìƒ˜í”Œë§ í’ˆì§ˆ ë¶„ì„
        unique_users = len(set(sampled_users))
        user_counts = {}
        for user_id in sampled_users:
            user_counts[user_id] = user_counts.get(user_id, 0) + 1
        
        print(f"[Buffer] ğŸ¯ Replacement sampling successful:")
        print(f"   Requested: {batch_size}")
        print(f"   Delivered: {len(images)}")
        print(f"   Unique users: {unique_users}")
        print(f"   Distribution: {dict(sorted(user_counts.items()))}")
        print(f"   Buffer utilization: {len(self.image_storage)} available samples")
        
        return images, labels

    def sample_without_replacement(self, batch_size: int):
        """
        ğŸ”§ ë¹„ë³µì› ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹) - í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
        
        NOTE: ì´ ë°©ì‹ì€ W2ML ë°°ì¹˜ í¬ê¸° ë¬¸ì œë¥¼ ì¼ìœ¼í‚´
        """
        if len(self.image_storage) == 0:
            return [], []
            
        num_samples = min(batch_size, len(self.image_storage))  # ì—¬ê¸°ì„œ ì œí•œ ë°œìƒ!
        sampled_indices = np.random.choice(len(self.image_storage), num_samples, replace=False)
        
        images = []
        labels = []
        for idx in sampled_indices:
            item = self.image_storage[idx]
            images.append(item['image'].clone())
            labels.append(item['user_id'])
        
        print(f"[Buffer] ğŸ“Š Non-replacement sampling:")
        print(f"   Requested: {batch_size}")
        print(f"   Available: {len(self.image_storage)}")
        print(f"   Delivered: {len(images)} (LIMITED)")
            
        return images, labels

    def get_diversity_stats(self):
        """ë²„í¼ ë‹¤ì–‘ì„± í†µê³„ ë°˜í™˜"""
        if len(self.image_storage) == 0:
            return {
                'total_samples': 0,
                'unique_users': 0,
                'user_distribution': {},
                'diversity_score': 0.0
            }
        
        user_counts = {}
        for item in self.image_storage:
            user_id = item['user_id']
            user_counts[user_id] = user_counts.get(user_id, 0) + 1
        
        unique_users = len(user_counts)
        diversity_score = unique_users / len(self.image_storage)
        
        return {
            'total_samples': len(self.image_storage),
            'unique_users': unique_users,
            'user_distribution': dict(sorted(user_counts.items())),
            'diversity_score': diversity_score
        }

    def save_state(self):
        """ë²„í¼ì˜ í˜„ì¬ ìƒíƒœë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        if self.faiss_index is None:
            print("[Buffer] No data to save.")
            return
            
        print(f"[Buffer] Saving replay buffer state to: {self.state_file}")
        
        data_to_save = {
            'faiss_index_data': faiss.serialize_index(self.faiss_index),
            'metadata': self.metadata,
            'image_storage': self.image_storage,
            'stored_embeddings': self.stored_embeddings  # ğŸ”§ FIX: ì„ë² ë”©ë„ ì €ì¥
        }
        
        with open(self.state_file, 'wb') as f:
            pickle.dump(data_to_save, f)
        
        # ë‹¤ì–‘ì„± í†µê³„ë„ ì €ì¥
        diversity_stats = self.get_diversity_stats()
        stats_file = self.storage_dir / 'buffer_diversity_stats.json'
        
        import json
        with open(stats_file, 'w') as f:
            json.dump(diversity_stats, f, indent=2)
        
        print(f"[Buffer] Saved {len(self.image_storage)} samples to disk.")
        print(f"[Buffer] Diversity stats: {diversity_stats['unique_users']} users, "
              f"{diversity_stats['diversity_score']:.2f} diversity score")

    def _load_state(self):
        """íŒŒì¼ì—ì„œ ë²„í¼ ìƒíƒœë¥¼ ë³µì›í•©ë‹ˆë‹¤."""
        if not self.state_file.exists():
            print(f"[Buffer] State file not found. Starting with an empty buffer.")
            return

        print(f"[Buffer] Loading replay buffer state from: {self.state_file}")
        try:
            with open(self.state_file, 'rb') as f:
                saved_data = pickle.load(f)
                
                # Faiss ì¸ë±ìŠ¤ ë³µì›
                self.faiss_index = faiss.deserialize_index(saved_data['faiss_index_data'])
                
                # ë©”íƒ€ë°ì´í„° ë³µì›
                self.metadata = saved_data['metadata']
                
                # ì´ë¯¸ì§€ ì €ì¥ì†Œë„ ë³µì›
                if 'image_storage' in saved_data:
                    self.image_storage = saved_data['image_storage']
                
                # ğŸ”§ FIX: ì €ì¥ëœ ì„ë² ë”©ë„ ë³µì›
                if 'stored_embeddings' in saved_data:
                    self.stored_embeddings = saved_data['stored_embeddings']
                else:
                    # ì´ì „ ë²„ì „ í˜¸í™˜ì„±: ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
                    self.stored_embeddings = []
                    print("[Buffer] Warning: No stored embeddings found in saved state.")
                    print("[Buffer] Buffer will work but culling may be limited until new samples are added.")
                
                # ë³µì›ëœ ë‹¤ì–‘ì„± í†µê³„ ì¶œë ¥
                diversity_stats = self.get_diversity_stats()
                print(f"[Buffer] Loaded {len(self.image_storage)} samples from disk.")
                print(f"[Buffer] Restored diversity: {diversity_stats['unique_users']} users")
                print(f"[Buffer] Embeddings loaded: {len(self.stored_embeddings)}")
                
        except Exception as e:
            print(f"[Buffer] Failed to load state: {e}. Starting with empty buffer.")
            self.faiss_index = None
            self.metadata = {}
            self.image_storage = []
            self.stored_embeddings = []  # ğŸ”§ FIX: ë¹ˆ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”