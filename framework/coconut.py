"""
=== COCONUT STAGE 2: TRUE CONTINUAL LEARNING WITH CHECKPOINT RESUME ===

ğŸ”§ COMPLETE REPLACEMENT FOR framework/coconut.py

DESIGN RATIONALE:
1. True continual learning with checkpoint resume
2. Complete system state preservation  
3. Automatic recovery from interruptions
4. Never lose learning progress
5. Incremental step counting with dataset position tracking

ğŸ¯ KEY FEATURES:
- Checkpoint = Model + Optimizer + Stats + Buffer + Dataset Position
- Auto-resume from last checkpoint
- Proper step counting that survives interruptions
- Safe checkpoint cleanup
- Full W2ML implementation with Faiss optimization
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import copy
import json
import pickle
import time
from pathlib import Path
from tqdm import tqdm
import datetime

# Faiss import with fallback
try:
    import faiss
    import numpy as np
    FAISS_AVAILABLE = True
    print("[System] ğŸš€ Faiss available - W2ML optimization enabled")
except ImportError:
    FAISS_AVAILABLE = False
    print("[System] âš ï¸ Faiss not found - using PyTorch fallback")

from models.ccnet_model import ccnet
from framework.replay_buffer import CoconutReplayBuffer
from framework.losses import CompleteW2MLSupConLoss
from datasets.palm_dataset import MyDataset
from torch.utils.data import DataLoader

class CoconutSystem:
    def __init__(self, config):
        """
        Continual Learning CoCoNut System
        
        COMPLETE REPLACEMENT - supports checkpoint resume and true continual learning
        """
        print("="*80)
        print("ğŸ¥¥ COCONUT STAGE 2: TRUE CONTINUAL LEARNING ADAPTATION")
        print("="*80)
        print("ğŸ”„ TRUE CONTINUAL LEARNING FEATURES:")
        print("   - Complete checkpoint saving (model + optimizer + stats + buffer)")
        print("   - Automatic resume from last checkpoint")
        print("   - True incremental learning")
        print("   - Never lose progress")
        print("   - Proper step counting across interruptions")
        print("="*80)
        
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"[System] Using device: {self.device}")
        print(f"[System] Faiss status: {'Available' if FAISS_AVAILABLE else 'Fallback mode'}")
        
        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì •
        self.checkpoint_dir = Path('/content/drive/MyDrive/CoCoNut_STAR/checkpoints')
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self._initialize_models()
        self._initialize_replay_buffer()
        self._initialize_w2ml_adaptive_learning()
        
        # í•™ìŠµ ìƒíƒœ ì´ˆê¸°í™”
        self.learner_step_count = 0 # ì§€ê¸ˆê¹Œì§€ í•™ìŠµí•œ step 
        self.global_dataset_index = 0  # ì „ì²´ ë°ì´í„°ì…‹ì—ì„œì˜ ìœ„ì¹˜ -> ì¤‘ë‹¨ ë³µì› ì‹œ í•„ìš”
        self._initialize_w2ml_stats() # ë‚œì´ë„ ê¸°ë°˜ ì†ì‹¤í•¨ìˆ˜ ì„±ëŠ¥ í†µê³„ ê¸°ë¡ìš© ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        
        # ì´ì „ì— í•™ìŠµ ì¤‘ë‹¨ ì‹œ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì› -> ê°€ì¥ ë§ˆì§€ë§‰ì— ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì™€ ìƒíƒœ ë³µêµ¬ / ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, í†µê³„, ë¦¬í”Œë ˆì´ ë²„í¼ ë“±ì„ ë³µêµ¬
        self._resume_from_latest_checkpoint()
        
        print(f"[System] ğŸ¥¥ Continual CoCoNut ready!")
        print(f"[System] Starting from step: {self.learner_step_count}")
        print(f"[System] Dataset position: {self.global_dataset_index}")

    def _initialize_models(self):
        # ì˜ˆì¸¡ê¸°ì™€ í•™ìŠµê¸° ëª¨ë¸ì„ ìƒì„±í•˜ê³  ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        print("[System] Initializing Predictor and Learner models...")
        cfg_model = self.config.palm_recognizer
        #########################################################
        # ì„¤ì • ê°ì²´(config)ì—ì„œ palm recognition ê´€ë ¨ í•˜ìœ„ ì„¤ì • 
        #
        # architecture: CCNet (default) ì‚¬ìš©í•  ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì´ë¦„ -> ccnet() í•¨ìˆ˜ë¡œ ëª¨ë¸ ìƒì„±ë¨
        # batch_size : í•™ìŠµ ì‹œ ì‚¬ìš©ë˜ëŠ” ë°°ì¹˜ í¬ê¸°
        # com_weight: 0.8 - ëª¨ë¸ ë‚´ë¶€ì—ì„œ ì‚¬ìš©ë˜ëŠ” weighting íŒŒë¼ë¯¸í„° -> ccnet ê³ ì • ê°€ì¤‘ì¹˜
        # feature_dimension: 2048 - íŠ¹ì§• ë²¡í„°ì˜ ì°¨ì› ìˆ˜ -> replay buffer, contrastive loss ë“± ì—¬ëŸ¬ ë¶€ë¶„ì—ì„œ ì‚¬ìš©ë¨
        # learning_rate: 0.001 
        # load_weights_folder: Stage 1 ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ê²½ë¡œ -> predictor/learner ëª¨ë¸ ì´ˆê¸°í™” ì‹œ ë¡œë“œë¨
        # num_classes: 600 ì‚¬ì „ í•™ìŠµ ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜(Tongji ë°ì´í„°ì…‹)
        #########################################################
        
        # ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±
        self.predictor_net = ccnet(num_classes=cfg_model.num_classes, weight=cfg_model.com_weight).to(self.device)
        self.learner_net = ccnet(num_classes=cfg_model.num_classes, weight=cfg_model.com_weight).to(self.device)
        
        # ì´ êµ¬ì¡°ëŠ” ë™ê¸°í™” ê°€ëŠ¥í•œ ì´ì¤‘ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¡œ, continual learningì— ìœ ë¦¬í•¨
        # â†’ learnerê°€ í•™ìŠµí•œ ë‚´ìš©ì„ ì¼ì • ì£¼ê¸°ë§ˆë‹¤ predictorë¡œ ë³µì‚¬(sync)
        
        # ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ (ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ì„ ë•Œë§Œ)
        weights_path = cfg_model.load_weights_folder # Stage 1ì—ì„œ ì €ì¥í•œ .pth ê²½ë¡œ ë¶ˆëŸ¬ì˜´
        print(f"[System] Loading pretrained weights from: {weights_path}") 
        try:
            self.predictor_net.load_state_dict(torch.load(weights_path, map_location=self.device))
            self.learner_net.load_state_dict(self.predictor_net.state_dict())
            # learner_netì€ predictor_netì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³µì‚¬í•´ì„œ ë™ì¼í•˜ê²Œ ì´ˆê¸°í™”
            print("[System] Successfully loaded pretrained weights (Stage 1 â†’ Stage 2)")
        except FileNotFoundError:
            print(f"[System] Pretrained weights not found. Starting with random weights.")
        except Exception as e:
            print(f"[System] Failed to load weights: {e}")
            
        self.predictor_net.eval() # predictor_net: í‰ê°€ëª¨ë“œ / ë³€í•˜ì§€ ì•ŠëŠ” ê¸°ì¤€ ëª¨ë¸, ì¸ì¦ ìš©ë„
        self.learner_net.train() # learner_net: í•™ìŠµëª¨ë“œ / ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë°›ì•„ ê³„ì† ì ì‘

    def _initialize_replay_buffer(self):
        """
        ë¦¬í”Œë ˆì´ ë²„í¼ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        ë²„í¼ê°€ ì‚¬ìš©í•  feature extractor(íŠ¹ì§• ì¶”ì¶œê¸°, ì¦‰ learner_net)ë¥¼ ë“±ë¡í•˜ì—¬
        ë‚˜ì¤‘ì— ìƒ˜í”Œ ì¶”ê°€(add), ìƒ˜í”Œë§(sample), í•˜ë“œ ìƒ˜í”Œ ë§ˆì´ë‹ ë“±ì˜ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.
        """
        print("[System] Initializing Replay Buffer...")
        cfg_buffer = self.config.replay_buffer
        cfg_model = self.config.palm_recognizer

        buffer_storage_path = Path(cfg_buffer.storage_path)
        
        self.replay_buffer = CoconutReplayBuffer(
            config=cfg_buffer,
            storage_dir=buffer_storage_path,
            feature_dimension=cfg_model.feature_dimension 
        )
        
        # ë¦¬í”Œë ˆì´ ë²„í¼ì— íŠ¹ì§• ì¶”ì¶œê¸° ì„¤ì •
        self.replay_buffer.set_feature_extractor(self.learner_net)

    def _initialize_w2ml_adaptive_learning(self):
        """Complete W2ML ê¸°ë°˜ ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        
        print("[System] ğŸ”¥ Initializing corrected W2ML...")
        
        cfg_model = self.config.palm_recognizer
        cfg_loss = self.config.loss
        
        # Adam ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.Adam(
            self.learner_net.parameters(), 
            lr=cfg_model.learning_rate
        )
        
        # ğŸ”¥ Complete W2ML ì†ì‹¤ í•¨ìˆ˜
        self.adaptive_contrastive_loss = CompleteW2MLSupConLoss(
            temperature=getattr(cfg_loss, 'w2ml_temperature', 0.07),
            hard_positive_weight=getattr(cfg_loss, 'hard_positive_weight', 2.0),
            hard_negative_weight=getattr(cfg_loss, 'hard_negative_weight', 2.0),
            similarity_threshold_pos=getattr(cfg_loss, 'similarity_threshold_pos', 0.1),
            similarity_threshold_neg=getattr(cfg_loss, 'similarity_threshold_neg', 0.7),
            negative_loss_weight=getattr(cfg_loss, 'negative_loss_weight', 0.3),
            enable_logging=getattr(cfg_loss, 'enable_w2ml_logging', True),
            use_faiss_optimization=FAISS_AVAILABLE,
            faiss_batch_threshold=6, # ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ ì´ ì´ìƒì¼ ë•Œë§Œ Faissë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì¡°ê±´ ì„¤ì •
            prefer_gpu_faiss=torch.cuda.is_available() # GPUì—ì„œ Faissë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆë‹¤ë©´ GPU ì‚¬ìš©
        )
        
        print("[System]   Initialized adaptive loss function")
        print(f"[System]  Configured batch size: {cfg_model.batch_size}")
        print(f"[System]  Hard positive threshold: {getattr(cfg_loss, 'similarity_threshold_pos', 0.1)}")
        print(f"[System]  Hard negative threshold: {getattr(cfg_loss, 'similarity_threshold_neg', 0.7)}")

    def _initialize_w2ml_stats(self):
        """W2ML í†µê³„ ì´ˆê¸°í™”"""
        self.w2ml_stats = {
            'hard_negative_count': 0,     # ì§€ê¸ˆê¹Œì§€ í•™ìŠµ ì¤‘ ê°ì§€ëœ hard negative pairì˜ ì´ ê°œìˆ˜
            'hard_positive_count': 0,     # ê°ì§€ëœ hard positive pairì˜ ì´ ê°œìˆ˜
            'total_learning_steps': 0,    # ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜ (ë°°ì¹˜ ë‹¨ìœ„ í•™ìŠµ ë°˜ë³µ íšŸìˆ˜)
            'difficulty_scores': [],      # ê° ë°°ì¹˜ì—ì„œì˜ í‰ê·  ë‚œì´ë„ ì ìˆ˜ (ì˜ˆ: cosine distance ê¸°ë°˜)
            'weight_amplifications': [],  # í•˜ë“œ ìƒ˜í”Œì— ë¶€ì—¬ëœ ê°€ì¤‘ì¹˜ ë°°ìœ¨ ê¸°ë¡ (ì˜ˆ: 2.0x, 1.5x ë“±)
            'faiss_speedups': [],         # FAISSë¥¼ ì‚¬ìš©í•œ ê²½ìš°, ê±°ë¦¬ ê³„ì‚°/ê²€ìƒ‰ ì‹œê°„ ì ˆì•½ë¥ 
            'processing_times': [],       # í•œ ë°°ì¹˜ ì²˜ë¦¬ì— ê±¸ë¦° ì‹œê°„ (ë‹¨ìœ„: ì´ˆ ë˜ëŠ” ms)
            'batch_sizes': [],            # ê° ë°°ì¹˜ì˜ ì‹¤ì œ ìƒ˜í”Œ ìˆ˜ (ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ ì‹œ ìœ ìš©)
            'detection_rates': []         # ê° ë°°ì¹˜ì—ì„œ í•˜ë“œ ìƒ˜í”Œì´ ê°ì§€ëœ ë¹„ìœ¨ (ex: 0.3 â†’ 30%)
        }

    def _resume_from_latest_checkpoint(self):
        """
        COCONUT STAGE 2 ì‹œìŠ¤í…œì˜ í•µì‹¬ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ì¸ ì¤‘ë‹¨ëœ í•™ìŠµì„ ë³µì›í•˜ëŠ” ë¡œì§ì…ë‹ˆë‹¤.
        ì‹¤ì œë¡œ í•™ìŠµì„ ëŠê³  ë‹¤ì‹œ ì‹œì‘í•  ë•Œ, ëª¨ë¸ ê°€ì¤‘ì¹˜ë¿ ì•„ë‹ˆë¼ í•™ìŠµ ìŠ¤í… ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ë²„í¼, í†µê³„ê¹Œì§€ 
        ëª¨ë‘ ë³µì›
        continual learningì˜ íšŒë³µì„±ì„ ìœ ì§€.
        """
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬ì—ì„œ checkpoint_step_ìˆ«ì.pth íŒ¨í„´ì˜ íŒŒì¼ì„ ëª¨ë‘ ì°¾ìŒ
        checkpoint_files = list(self.checkpoint_dir.glob('checkpoint_step_*.pth'))
        
        # ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ë‹¤ë©´ ìƒˆë¡œ í•™ìŠµ ì‹œì‘
        if not checkpoint_files:
            print("[Resume] ğŸ“‚ No checkpoints found - starting fresh")
            return
        
        # ê°€ì¥ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸° -> íŒŒì¼ëª…ì—ì„œ step ìˆ«ìë¥¼ ì¶”ì¶œí•˜ì—¬ ê°€ì¥ í° step ìˆ˜ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„ íƒ
        latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.stem.split('_')[-1]))
        step_num = int(latest_checkpoint.stem.split('_')[-1])
        
        print(f"[Resume] ğŸ”„ Found checkpoint: {latest_checkpoint.name}")
        print(f"[Resume] ğŸ“ Resuming from step: {step_num}")
        
        try:
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = torch.load(latest_checkpoint, map_location=self.device)
            
            # ëª¨ë¸ ìƒíƒœ ë³µì›
            self.learner_net.load_state_dict(checkpoint['learner_state_dict'])
            self.predictor_net.load_state_dict(checkpoint['predictor_state_dict'])
            
            # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë³µì›
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # Stage 2ì˜ learnerì™€ predictor ëª¨ë¸, ì˜µí‹°ë§ˆì´ì € ë³µì›
            self.learner_step_count = checkpoint['step_count']
            self.global_dataset_index = checkpoint.get('global_dataset_index', 0)
            self.w2ml_stats = checkpoint['w2ml_stats']
            
            # ë¦¬í”Œë ˆì´ ë²„í¼ ìƒíƒœ ë³µì› (ë³„ë„ íŒŒì¼) / ë²„í¼ëŠ” ë³„ë„ë¡œ .pklë¡œ ì €ì¥ë˜ì–´ ìˆìŒ
            buffer_checkpoint = self.checkpoint_dir / f'buffer_step_{step_num}.pkl'
            if buffer_checkpoint.exists():
                with open(buffer_checkpoint, 'rb') as f:
                    buffer_data = pickle.load(f)
                    self.replay_buffer.image_storage = buffer_data['image_storage']
                    self.replay_buffer.stored_embeddings = buffer_data.get('stored_embeddings', [])
                    self.replay_buffer.metadata = buffer_data['metadata']
                    if buffer_data.get('faiss_index_data'):
                        self.replay_buffer.faiss_index = faiss.deserialize_index(buffer_data['faiss_index_data'])
                    else:
                        self.replay_buffer.faiss_index = None
            
            print(f"[Resume] Successfully resumed from step {self.learner_step_count}")
            print(f"[Resume] stats restored:")
            print(f"   - Hard negatives: {self.w2ml_stats['hard_negative_count']}")
            print(f"   - Hard positives: {self.w2ml_stats['hard_positive_count']}")
            print(f"   - Total learning steps: {self.w2ml_stats['total_learning_steps']}")
            print(f"   - Buffer size: {len(self.replay_buffer.image_storage)}")
            print(f"   - Dataset position: {self.global_dataset_index}")
        
        # ë³µì› ì‹¤íŒ¨ ì‹œ, ì „ì²´ ì´ˆê¸°í™”í•´ì„œ ê¹¨ë—í•œ ì‹œì‘ì´ ë˜ë„ë¡ í•¨
        except Exception as e:
            print(f"[Resume] âŒ Failed to resume: {e}")
            print(f"[Resume] ğŸ”„ Starting fresh instead")
            self.learner_step_count = 0
            self.global_dataset_index = 0

    def run_experiment(self):
        """ COCONUT STAGE 2 ì‹œìŠ¤í…œì—ì„œ continual learning ì‹¤í—˜ì˜ ì£¼ ë£¨í”„ì…ë‹ˆë‹¤. """
        print(f"[System] Starting continual learning from step {self.learner_step_count}...")

        # íƒ€ê²Ÿ ë°ì´í„°ì…‹ ì¤€ë¹„
        cfg_dataset = self.config.dataset
        target_dataset = MyDataset(txt=str(cfg_dataset.dataset_path), train=False)
        target_dataloader = DataLoader(target_dataset, batch_size=1, shuffle=False) # ì‹¤ì œ ì¸ì¦ ì‹œìŠ¤í…œì€ í•œ ì¥ì˜ ì‚¬ì§„ìœ¼ë¡œ ì¸ì¦í•˜ë‹ˆ ë°°ì¹˜ ì‚¬ì´ì¦ˆ 1
        
        #  ì´ë¯¸ ì²˜ë¦¬í•œ ë°ì´í„°ë“¤ì€ ê±´ë„ˆë›°ê¸°
        dataset_list = list(target_dataloader)
        total_steps = len(dataset_list)
        
        # global_dataset_indexëŠ” ë§ˆì§€ë§‰ê¹Œì§€ í•™ìŠµí•œ ë°ì´í„° ì¸ë±ìŠ¤, ë§Œì•½ ì´ì „ì— ì´ë¯¸ ë‹¤ í•™ìŠµí–ˆë‹¤ë©´ ë°˜ë³µ í•™ìŠµ ë°©ì§€ìš©ìœ¼ë¡œ ì¢…ë£Œ
        if self.global_dataset_index >= total_steps:
            print(f"[System] All data already processed! ({self.global_dataset_index}/{total_steps})")
            return
        
        # ë‚¨ì€ ë°ì´í„° ê°œìˆ˜ë¥¼ ì‚¬ìš©ìì—ê²Œ ì•Œë ¤ì¤Œ
        print(f"[System] Resuming from dataset position {self.global_dataset_index}/{total_steps}")
        print(f"[System] Remaining data: {total_steps - self.global_dataset_index}")

        #  ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„° ê±´ë„ˆë›°ê³  ì´ì–´ì„œ ì‹¤í–‰ -> ì´ì–´ì„œ í•™ìŠµí•  ë°ì´í„°ë§Œ ì¶”ì¶œ
        remaining_data = dataset_list[self.global_dataset_index:]
        
        for data_offset, (datas, user_id) in enumerate(tqdm(remaining_data, desc="True Continual W2ML")):
            
            # ì „ì²´ ë°ì´í„°ì…‹ì—ì„œì˜ í˜„ì¬ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
            self.global_dataset_index = self.global_dataset_index + data_offset
            
            primary_image = datas[0].squeeze(0)
            user_id = user_id.item()

            # í•œ í”„ë ˆì„ ì²˜ë¦¬
            self.process_single_frame_w2ml(primary_image, user_id)

            # ì„¤ì •ëœ ë¹ˆë„ì— ë”°ë¼ ì™„ì „í•œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_frequency = getattr(self.config.continual_learner, 'intermediate_save_frequency', 50)
            if save_frequency > 0 and self.learner_step_count > 0 and self.learner_step_count % save_frequency == 0:
                self._save_complete_checkpoint()

            # ì¤‘ê°„ ê²°ê³¼ ë¡œê¹…
            if self.learner_step_count % 100 == 0 and self.learner_step_count > 0:
                self._log_w2ml_progress(self.global_dataset_index, total_steps)

        # ë§ˆì§€ë§‰ ë°ì´í„° ì²˜ë¦¬ í›„ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        self.global_dataset_index = total_steps

        # ì‹¤í—˜ ì¢…ë£Œ í›„ ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        print("\n[System] Continual Learning experiment finished.")
        self._save_complete_checkpoint()
        self._final_w2ml_analysis()
        self.save_system_state()

    def process_single_frame_w2ml(self, image: torch.Tensor, user_id: int):
        """
        ë‹¨ì¼ í”„ë ˆì„(ìƒ˜í”Œ)ì„ ì²˜ë¦¬í•˜ì—¬ ë‚œì´ë„ ê¸°ë°˜ í•™ìŠµì„ ì¡°ê±´ë¶€ë¡œ ìˆ˜í–‰í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ì…ë‹ˆë‹¤.
        ì¦‰, 1ê°œì˜ palm ì´ë¯¸ì§€ê°€ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤

        ì˜ˆì¸¡ê¸°/í•™ìŠµê¸°ì˜ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ê³ 
        ë¦¬í”Œë ˆì´ ë²„í¼ì— ì¶”ê°€í•˜ë©°
        ì¡°ê±´ì´ ì¶©ì¡±ë˜ë©´ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.
        """
        image = image.to(self.device)

        # 1. ì˜ˆì¸¡ê¸°ë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì¸ì¦, ì˜ˆì¸¡ê¸°ëŠ” ì¶”ë¡ ë§Œ í•˜ê³  í•™ìŠµì—ëŠ” ê´€ì—¬ ì•ˆ í•¨
        self.predictor_net.eval()
        with torch.no_grad():
            embedding_from_predictor = self.predictor_net.getFeatureCode(image)
        
        # 2. í•™ìŠµê¸°ë¥¼ í†µí•œ ìµœì‹  íŠ¹ì§• ì¶”ì¶œ, ë™ì¼í•˜ê²Œ í”¼ì²˜ ì¶”ì¶œë§Œ í•˜ê³  ì´í›„ì— í•™ìŠµê¸°ë¡œ ëŒì•„ê°, ì—¬ê¸°ì„œ ì¶”ì¶œí•œ embeddingì€ ë¦¬í”Œë ˆì´ ë²„í¼ ë‚´ ìœ ì‚¬ë„ ê³„ì‚° ë“±ì— ì‚¬ìš©ë¨
        self.learner_net.eval()
        with torch.no_grad():
            latest_embedding = self.learner_net.getFeatureCode(image)
        self.learner_net.train()
        
        # 3. ë¦¬í”Œë ˆì´ ë²„í¼ì— ì¶”ê°€
        self.replay_buffer.add(image, user_id)
        
        # 4. í˜„ì¬ ë²„í¼ì— ì €ì¥ëœ ì „ì²´ ìƒ˜í”Œ ìˆ˜ì™€ user_idê°€ ì„œë¡œ ë‹¤ë¥¸ ê³ ìœ  ì‚¬ìš©ì ìˆ˜ë¥¼ ì¹´ìš´íŠ¸
        buffer_size = len(self.replay_buffer.image_storage)
        unique_users = len(set([item['user_id'] for item in self.replay_buffer.image_storage]))
        
        # ìµœì†Œ ì¡°ê±´: 2ëª… ì´ìƒì˜ ì‚¬ìš©ì (ëŒ€ì¡°í•™ìŠµì„ ìœ„í•œ ìµœì†Œ ë‹¤ì–‘ì„±), contrastive learningì€ anchor-positive-negative ê´€ê³„ê°€ í•„ìš”
        if unique_users < 2:
            print(f"[W2ML] ğŸ“Š Waiting for diversity (Dataset pos: {self.global_dataset_index}):")
            print(f"   Buffer size: {buffer_size}")
            print(f"   Unique users: {unique_users}/2 minimum")
            print(f"   Need more diverse users for contrastive learning")
            return
        
        # 5. ì²« ë²ˆì§¸ í•™ìŠµ ì‹œì‘ ì•Œë¦¼
        if unique_users == 2 and buffer_size <= 3:
            print(f"\nğŸ‰ [W2ML] TRUE CONTINUAL LEARNING ACTIVATED!")
            print(f"   Minimum diversity achieved: {unique_users} users")
            print(f"   Target batch size: {self.config.palm_recognizer.batch_size}")
            print(f"   Will use sample with replacement for full batch")
        
        # 6. ë‚œì´ë„ ê¸°ë°˜ ì ì‘í•™ìŠµ ì‹¤í–‰
        self._trigger_corrected_w2ml_learning(image, user_id)

    def _trigger_corrected_w2ml_learning(self, new_image, new_user_id):
        """
        COCONUT STAGE 2 ì‹œìŠ¤í…œì—ì„œ í•œ ì´ë¯¸ì§€ì— ëŒ€í•´ ì‹¤ì œ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” í•µì‹¬ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
        """
        
        # í•™ìŠµ ìŠ¤í… ì¦ê°€ (í•™ìŠµì´ ì‹¤ì œë¡œ ë°œìƒí•  ë•Œë§Œ)
        self.learner_step_count += 1
        
        print(f"[W2ML] {'='*60}")
        print(f"[W2ML] CORRECTED W2ML STEP {self.learner_step_count}")
        print(f"[W2ML] {'='*60}")
        
        cfg_learner = self.config.continual_learner
        cfg_model = self.config.palm_recognizer
        target_batch_size = cfg_model.batch_size

        #    batch_sizeëŠ” ì˜ˆ: 10ì´ë¼ë©´, ìƒˆ ì´ë¯¸ì§€ 1ì¥ + ë²„í¼ì—ì„œ 9ê°œ ìƒ˜í”Œë¡œ êµ¬ì„±
        #    with replacement: ìƒ˜í”Œì´ ë¶€ì¡±í•´ë„ ì¤‘ë³µ í—ˆìš©ìœ¼ë¡œ ë½‘ì„ ìˆ˜ ìˆìŒ
        replay_count = target_batch_size - 1
        
        # ë³µì› ì¶”ì¶œë¡œ ì¶©ë¶„í•œ ê°œìˆ˜ í™•ë³´
        replay_images, replay_labels = self.replay_buffer.sample_with_replacement(replay_count)
        
        all_images = [new_image] + replay_images
        all_labels = [new_user_id] + replay_labels
        
        # ê²€ì¦: ì‹¤ì œ ë°°ì¹˜ í¬ê¸° í™•ì¸
        actual_batch_size = len(all_images)
        
        print(f"[W2ML] CORRECTED Batch Analysis:")
        print(f"   Target batch size: {target_batch_size}")
        print(f"   Actual batch size: {actual_batch_size}")
        print(f"   Size match: {actual_batch_size == target_batch_size}")
        print(f"   Current user: {new_user_id}")
        print(f"   Replay samples: {len(replay_images)}")
        
        # ëŒ€ì¡° ìŒ ë¶„ì„, SupConì—ì„œëŠ” ëª¨ë“  ì´ë¯¸ì§€ ê°„ ìŒì„ ê³ ë ¤í•˜ê¸° ë•Œë¬¸ì—, N x (N-1)ì´ ë¨.
        total_pairs = actual_batch_size * (actual_batch_size - 1)
        print(f"   Total contrastive pairs: {total_pairs}")
        
        # ë‹¤ì–‘ì„± ë¶„ì„, ë¼ë²¨ì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° â†’ ê³ ìœ  ì‚¬ìš©ì ìˆ˜ ì¸¡ì •
        unique_users = len(set(all_labels))
        user_distribution = {}
        for label in all_labels:
            user_distribution[label] = user_distribution.get(label, 0) + 1
        
        print(f"   Unique users: {unique_users}")
        print(f"   User distribution: {dict(sorted(user_distribution.items()))}")
        
        # Faiss ìµœì í™” ìƒíƒœ
        faiss_optimized = actual_batch_size >= 6
        print(f"   Faiss optimization: {'ACTIVE' if faiss_optimized else 'FALLBACK'}")
        
        # W2ML ì ì‘ ì—í¬í¬ë“¤ ì‹¤í–‰
        total_loss = 0.0
        total_hard_negatives = 0
        total_hard_positives = 0
        total_weight_amplification = 0.0
        
        processing_start = time.time()
        
        # configì— ì„¤ì •ëœ ì—í¬í¬ ìˆ˜ë§Œí¼ ì ì‘ í•™ìŠµ ìˆ˜í–‰
        for epoch in range(cfg_learner.adaptation_epochs):
            print(f"[W2ML] ğŸ”„ Adaptation epoch {epoch+1}/{cfg_learner.adaptation_epochs}")
            
            epoch_loss, hard_neg_count, hard_pos_count, weight_amp = self._run_corrected_w2ml_step(all_images, all_labels)
            total_loss += epoch_loss
            total_hard_negatives += hard_neg_count
            total_hard_positives += hard_pos_count
            total_weight_amplification += weight_amp
        
        processing_time = time.time() - processing_start
        average_loss = total_loss / cfg_learner.adaptation_epochs
        average_weight_amp = total_weight_amplification / cfg_learner.adaptation_epochs
        
        # Hard sample íƒì§€ìœ¨ ê³„ì‚°
        hard_detection_rate = 0
        if total_pairs > 0:
            hard_detection_rate = (total_hard_negatives + total_hard_positives) / total_pairs * 100
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.w2ml_stats['total_learning_steps'] += 1
        self.w2ml_stats['hard_negative_count'] += total_hard_negatives
        self.w2ml_stats['hard_positive_count'] += total_hard_positives
        self.w2ml_stats['difficulty_scores'].append(average_loss)
        self.w2ml_stats['weight_amplifications'].append(average_weight_amp)
        self.w2ml_stats['processing_times'].append(processing_time)
        self.w2ml_stats['batch_sizes'].append(actual_batch_size)
        self.w2ml_stats['detection_rates'].append(hard_detection_rate)
        
        print(f"[W2ML] ğŸ“Š CORRECTED Step {self.learner_step_count} Results:")
        print(f"   Average loss: {average_loss:.6f}")
        print(f"   Hard negatives: {total_hard_negatives}")
        print(f"   Hard positives: {total_hard_positives}")
        print(f"   Detection rate: {hard_detection_rate:.1f}%")
        print(f"   Weight amplification: {average_weight_amp:.2f}x")
        print(f"   Processing time: {processing_time*1000:.2f}ms")
        print(f"   Estimated speedup: {estimated_speedup:.0f}x")
        print(f"   W2ML performance: {performance_level}")
        
        # ğŸ”¥ ì„±ëŠ¥ í‰ê°€
        if hard_detection_rate > 15:
            print(f"   ğŸ‰ EXCELLENT hard sample detection!")
        elif hard_detection_rate > 10:
            print(f"   âœ… GOOD hard sample detection")
        elif hard_detection_rate > 5:
            print(f"   ğŸ‘ MODERATE hard sample detection")
        else:
            print(f"   âš ï¸ LOW detection - thresholds may need adjustment")
        
        # ëª¨ë¸ ë™ê¸°í™” ì²´í¬
        if self.learner_step_count % cfg_learner.sync_frequency == 0:
            self._sync_weights_with_corrected_analysis()

    def _run_corrected_w2ml_step(self, images: list, labels: list):
        """
        ğŸ”§ ìˆ˜ì •ëœ W2ML í•™ìŠµ ìŠ¤í… - gradient ì•ˆì „ì„± ë³´ì¥
        """
        
        print(f"[W2ML] ğŸ§  Processing {len(images)} samples with corrected W2ML")
        
        # 1. í•™ìŠµì„ ìœ„í•´ train ëª¨ë“œ ì„¤ì •
        self.learner_net.train()
        self.optimizer.zero_grad()
        
        # 2. ì„ë² ë”© ì¶”ì¶œ (requires_grad=True ìœ ì§€)
        embeddings = []
        for i, img in enumerate(images):
            img = img.to(self.device)
            if len(img.shape) == 3:
                img = img.unsqueeze(0)
            
            # Forward pass with gradient computation
            embedding = self.learner_net.getFeatureCode(img)
            embeddings.append(embedding)
        
        # 3. ë°°ì¹˜ í…ì„œ êµ¬ì„±
        embeddings_tensor = torch.cat(embeddings, dim=0)  # [batch_size, feature_dim]
        labels_tensor = torch.tensor(labels, dtype=torch.long, device=self.device)
        
        # 4. W2ML ì†ì‹¤ ê³„ì‚°
        embeddings_for_loss = embeddings_tensor.unsqueeze(1)  # [batch_size, 1, feature_dim]
        
        print("[W2ML] ğŸ¯ Computing corrected W2ML loss...")
        
        # ğŸ”§ Hard sample ì¹´ìš´íŠ¸ ì‹œ detach ì‚¬ìš© (í†µê³„ìš©ì´ë¯€ë¡œ gradient ë¶ˆí•„ìš”)
        with torch.no_grad():
            hard_neg_count, hard_pos_count = self._count_hard_samples_corrected(
                embeddings_tensor.detach(), labels_tensor.detach()
            )
            
            # ê°€ì¤‘ì¹˜ ì¦í­ ê³„ì‚° (í†µê³„ìš©)
            weight_amplification = self._calculate_weight_amplification(
                embeddings_tensor.detach(), labels_tensor.detach()
            )
        
        # ì‹¤ì œ ì†ì‹¤ ê³„ì‚° (gradient í•„ìš”)
        loss = self.adaptive_contrastive_loss(embeddings_for_loss, labels_tensor)
        
        # ğŸ”§ ì—­ì „íŒŒ
        if loss.requires_grad:
            loss.backward()
            self.optimizer.step()
            print("[W2ML] âœ… Gradient update completed")
        else:
            print("[W2ML] âš ï¸ No gradient - loss computation issue")
        
        print(f"[W2ML] âœ… Corrected Loss: {loss.item():.6f}")
        
        return loss.item(), hard_neg_count, hard_pos_count, weight_amplification

    def _count_hard_samples_corrected(self, embeddings, labels):
        """
        ğŸ”§ ìˆ˜ì •ëœ í•˜ë“œ ìƒ˜í”Œ íƒì§€ - ì˜¬ë°”ë¥¸ ì„ê³„ê°’ ì‚¬ìš©
        """
        batch_size = embeddings.shape[0]
        
        if not FAISS_AVAILABLE or batch_size < 6:
            return self._count_hard_samples_pytorch_corrected(embeddings, labels)
        
        return self._count_hard_samples_with_faiss_corrected(embeddings, labels)

    def _count_hard_samples_with_faiss_corrected(self, embeddings, labels):
        """ğŸ”§ ìˆ˜ì •ëœ Faiss í•˜ë“œ ìƒ˜í”Œ íƒì§€ - gradient ë¬¸ì œ í•´ê²°"""
        
        import time
        start_time = time.time()
        
        batch_size = embeddings.shape[0]
        
        # ğŸ”§ í•µì‹¬ ìˆ˜ì •: detach() ì¶”ê°€
        normalized_embeddings = F.normalize(embeddings, dim=1)
        embeddings_np = normalized_embeddings.detach().cpu().numpy().astype('float32')  # detach() ì¶”ê°€!
        labels_np = labels.detach().cpu().numpy()  # labelsë„ detach() ì¶”ê°€!
        
        # Faiss ì¸ë±ìŠ¤ ìƒì„± ë° ìœ ì‚¬ë„ ê³„ì‚°
        index = faiss.IndexFlatIP(embeddings_np.shape[1])
        index.add(embeddings_np)
        similarities, _ = index.search(embeddings_np, k=batch_size)
        
        # ìˆ˜ì •ëœ í•˜ë“œ ìƒ˜í”Œ ë¶„ì„
        hard_pos_count, hard_neg_count = self._analyze_similarities_numpy_corrected(similarities, labels_np)
        
        return hard_neg_count, hard_pos_count

    def _count_hard_samples_pytorch_corrected(self, embeddings, labels):
        """ğŸ”§ ìˆ˜ì •ëœ PyTorch í•˜ë“œ ìƒ˜í”Œ íƒì§€ - gradient ë¬¸ì œ í•´ê²°"""
        batch_size = embeddings.shape[0]
        device = embeddings.device
      
        # ğŸ”§ ìˆ˜ì •: detach()ë¥¼ ì‚¬ìš©í•˜ì—¬ gradient ì—°ê²° ëŠê¸°
        with torch.no_grad():  # ì¶”ê°€ ë³´í˜¸
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
            normalized_embeddings = F.normalize(embeddings.detach(), dim=1)  # detach() ì¶”ê°€!
            similarity_matrix = torch.mm(normalized_embeddings, normalized_embeddings.T)
            
            # ë¼ë²¨ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤
            labels_detached = labels.detach()  # detach() ì¶”ê°€!
            labels_expanded = labels_detached.unsqueeze(1)
            same_user_matrix = (labels_expanded == labels_expanded.T)
            
            # ëŒ€ê°ì„  ì œê±°
            eye_mask = torch.eye(batch_size, device=device).bool()
            valid_pairs_mask = ~eye_mask
            
            # ğŸ”§ ìˆ˜ì •ëœ í•˜ë“œ ìƒ˜í”Œ ì¡°ê±´ (ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¨ ì„ê³„ê°’ ì‚¬ìš©)
            pos_threshold = self.adaptive_contrastive_loss.similarity_threshold_pos
            neg_threshold = self.adaptive_contrastive_loss.similarity_threshold_neg
            
            hard_positive_mask = same_user_matrix & (similarity_matrix < pos_threshold) & valid_pairs_mask
            hard_negative_mask = (~same_user_matrix) & (similarity_matrix > neg_threshold) & valid_pairs_mask
            
            # ì¹´ìš´íŠ¸
            hard_positive_count = hard_positive_mask.sum().item()
            hard_negative_count = hard_negative_mask.sum().item()
        
        print(f"[PyTorch] ğŸ”„ Corrected mode: {batch_size}Â² pairs, pos_th={pos_threshold}, neg_th={neg_threshold}")
        
        return hard_negative_count, hard_positive_count

    def _analyze_similarities_numpy_corrected(self, similarities, labels_np):
        """
        NumPy í•˜ë“œ ìƒ˜í”Œ ë¶„ì„
        NumPy ê¸°ë°˜ìœ¼ë¡œ í•˜ë“œ ìƒ˜í”Œ(positive/negative)ì„ íƒì§€í•˜ëŠ” ë¡œì§ìœ¼ë¡œ, 
        FAISSë¥¼ í†µí•´ ìœ ì‚¬ë„(similarity) í–‰ë ¬ì„ ì–»ì€ í›„, í•˜ë“œ ìƒ˜í”Œì˜ ê°œìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë§Œë“¬.
        """
        batch_size = len(labels_np)
        
        # ë¼ë²¨ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤
        same_user_matrix = (labels_np[:, np.newaxis] == labels_np[np.newaxis, :])
        
        # ëŒ€ê°ì„  ì œê±°
        eye_mask = np.eye(batch_size, dtype=bool)
        valid_pairs = ~eye_mask
        
        # ğŸ”§ ìˆ˜ì •ëœ ì„ê³„ê°’ ì‚¬ìš©
        pos_threshold = self.adaptive_contrastive_loss.similarity_threshold_pos
        neg_threshold = self.adaptive_contrastive_loss.similarity_threshold_neg
        
        # í•˜ë“œ ìƒ˜í”Œ ì¡°ê±´
        hard_positive_mask = same_user_matrix & (similarities < pos_threshold) & valid_pairs
        hard_negative_mask = (~same_user_matrix) & (similarities > neg_threshold) & valid_pairs
        
        # ì¹´ìš´íŠ¸
        hard_positive_count = int(hard_positive_mask.sum())
        hard_negative_count = int(hard_negative_mask.sum())
        
        return hard_positive_count, hard_negative_count

    def _calculate_weight_amplification(self, embeddings, labels):
        """í•™ìŠµ ê³¼ì •ì—ì„œ í˜„ì¬ ë°°ì¹˜ê°€ ì–¼ë§ˆë‚˜ "ì–´ë ¤ìš´" ìƒ˜í”Œë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ì§€í‘œë¥¼ ì‚°ì¶œ"""
        batch_size = embeddings.shape[0]
        total_pairs = batch_size * (batch_size - 1)
        
        # ğŸ”§ ìˆ˜ì •: detach()ë¥¼ ì‚¬ìš©í•˜ì—¬ gradient ë¬¸ì œ ë°©ì§€
        with torch.no_grad():
            hard_neg, hard_pos = self._count_hard_samples_corrected(embeddings.detach(), labels.detach())
        
        hard_ratio = (hard_neg + hard_pos) / total_pairs if total_pairs > 0 else 0
        
        # ê°€ì¤‘ì¹˜ ì¦í­ ê³„ì‚°
        avg_hard_weight = (self.adaptive_contrastive_loss.hard_negative_weight + 
                          self.adaptive_contrastive_loss.hard_positive_weight) / 2
        amplification = 1.0 + (hard_ratio * avg_hard_weight)
        
        return amplification

    def _sync_weights_with_corrected_analysis(self):
        """
        í•™ìŠµê¸°(learner_net)ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì˜ˆì¸¡ê¸°(predictor_net)ë¡œ ë³µì‚¬
        ì˜ˆì¸¡ê¸°ëŠ” ì¶”ë¡  ì‹œ ì‚¬ìš©ë˜ë¯€ë¡œ eval() ëª¨ë“œ ì„¤ì •
        ìµœê·¼ W2ML í•™ìŠµ ì„±ëŠ¥ì„ ìš”ì•½ ë¶„ì„ ë° ì¶œë ¥
        ë¶„ì„ ê²°ê³¼ëŠ” ë””ë²„ê¹…, ì¡°ì •, ë¦¬í¬íŠ¸ ì‘ì„± ë“±ì— ë„ì›€ë¨
        """
        
        self.predictor_net.load_state_dict(self.learner_net.state_dict())
        self.predictor_net.eval()
        
        print(f"\n[W2ML Sync] CORRECTED SYNCHRONIZATION")
        print(f"[W2ML Sync] {'='*60}")
        
        # ì„±ëŠ¥ ë¶„ì„
        recent_steps = min(10, len(self.w2ml_stats['difficulty_scores']))
        if recent_steps > 0:
            recent_scores = self.w2ml_stats['difficulty_scores'][-recent_steps:]
            recent_detection_rates = self.w2ml_stats['detection_rates'][-recent_steps:]
            recent_batch_sizes = self.w2ml_stats['batch_sizes'][-recent_steps:]
            
            avg_difficulty = sum(recent_scores) / len(recent_scores)
            avg_detection = sum(recent_detection_rates) / len(recent_detection_rates)
            avg_batch_size = sum(recent_batch_sizes) / len(recent_batch_sizes)
            
            print(f"[W2ML Sync] ğŸ“Š Recent {recent_steps} steps analysis:")
            print(f"   Average difficulty: {avg_difficulty:.6f}")
            print(f"   Average detection rate: {avg_detection:.1f}%")
            print(f"   Average batch size: {avg_batch_size:.1f}")
            print(f"   Total hard negatives: {self.w2ml_stats['hard_negative_count']}")
            print(f"   Total hard positives: {self.w2ml_stats['hard_positive_count']}")
            
        
        print(f"[W2ML Sync] Corrected predictor updated!")
        print(f"[W2ML Sync] Full batch size W2ML learning active!")
        print(f"[W2ML Sync] {'='*60}\n")

    def _save_complete_checkpoint(self):
        """ğŸ”„ ì™„ì „í•œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ëª¨ë¸ + ì˜µí‹°ë§ˆì´ì € + ëª¨ë“  ìƒíƒœ)"""
        
        step = self.learner_step_count
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì¤€ë¹„
        checkpoint = {
            'step_count': step,
            'global_dataset_index': self.global_dataset_index,
            'timestamp': timestamp,
            'learner_state_dict': self.learner_net.state_dict(),
            'predictor_state_dict': self.predictor_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'w2ml_stats': self.w2ml_stats,
            'config_info': {
                'batch_size': self.config.palm_recognizer.batch_size,
                'learning_rate': self.config.palm_recognizer.learning_rate,
                'pos_threshold': self.adaptive_contrastive_loss.similarity_threshold_pos,
                'neg_threshold': self.adaptive_contrastive_loss.similarity_threshold_neg,
            }
        }
        
        # ë©”ì¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = self.checkpoint_dir / f'checkpoint_step_{step}.pth'
        torch.save(checkpoint, checkpoint_path)
        
        # ë¦¬í”Œë ˆì´ ë²„í¼ ìƒíƒœ ì €ì¥ (ë³„ë„ íŒŒì¼)
        buffer_data = {
            'image_storage': self.replay_buffer.image_storage,
            'stored_embeddings': getattr(self.replay_buffer, 'stored_embeddings', []),
            'metadata': self.replay_buffer.metadata,
            'faiss_index_data': faiss.serialize_index(self.replay_buffer.faiss_index) if self.replay_buffer.faiss_index else None
        }
        buffer_path = self.checkpoint_dir / f'buffer_step_{step}.pkl'
        with open(buffer_path, 'wb') as f:
            pickle.dump(buffer_data, f)
        
        # ìƒì„¸ í†µê³„ ì €ì¥
        stats_data = {
            'step': step,
            'global_dataset_index': self.global_dataset_index,
            'timestamp': timestamp,
            'learning_progress': {
                'total_steps': self.w2ml_stats['total_learning_steps'],
                'hard_negative_count': self.w2ml_stats['hard_negative_count'],
                'hard_positive_count': self.w2ml_stats['hard_positive_count'],
                'avg_detection_rate': sum(self.w2ml_stats['detection_rates'][-10:]) / min(10, len(self.w2ml_stats['detection_rates'])) if self.w2ml_stats['detection_rates'] else 0,
                'avg_batch_size': sum(self.w2ml_stats['batch_sizes'][-10:]) / min(10, len(self.w2ml_stats['batch_sizes'])) if self.w2ml_stats['batch_sizes'] else 0,
            },
            'buffer_status': {
                'size': len(self.replay_buffer.image_storage),
                'diversity': len(set([item['user_id'] for item in self.replay_buffer.image_storage])),
                'max_size': self.replay_buffer.buffer_size
            }
        }
        
        stats_path = self.checkpoint_dir / f'stats_step_{step}.json'
        with open(stats_path, 'w') as f:
            json.dump(stats_data, f, indent=2)
        
        # ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ (ìµœê·¼ 5ê°œë§Œ ìœ ì§€)
        self._cleanup_old_checkpoints()
        
        print(f"[Checkpoint] ğŸ’¾ Complete checkpoint saved:")
        print(f"   ğŸ“ Model: checkpoint_step_{step}.pth")
        print(f"   ğŸ“ Buffer: buffer_step_{step}.pkl") 
        print(f"   ğŸ“ Stats: stats_step_{step}.json")
        print(f"   ğŸ“ Dataset position: {self.global_dataset_index}")
        print(f"   ğŸ¯ Total hard samples: {self.w2ml_stats['hard_negative_count'] + self.w2ml_stats['hard_positive_count']}")

    def _cleanup_old_checkpoints(self, keep_last=5):
        """ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ë“¤ ì •ë¦¬"""
        
        checkpoint_files = list(self.checkpoint_dir.glob('checkpoint_step_*.pth'))
        if len(checkpoint_files) <= keep_last:
            return
        
        # ìŠ¤í… ë²ˆí˜¸ë¡œ ì •ë ¬í•˜ê³  ì˜¤ë˜ëœ ê²ƒë“¤ ì‚­ì œ
        checkpoint_files.sort(key=lambda x: int(x.stem.split('_')[-1]))
        files_to_delete = checkpoint_files[:-keep_last]
        
        for file_path in files_to_delete:
            step_num = int(file_path.stem.split('_')[-1])
            
            # ê´€ë ¨ íŒŒì¼ë“¤ ëª¨ë‘ ì‚­ì œ
            file_path.unlink()  # checkpoint_step_X.pth
            
            buffer_file = self.checkpoint_dir / f'buffer_step_{step_num}.pkl'
            if buffer_file.exists():
                buffer_file.unlink()
                
            stats_file = self.checkpoint_dir / f'stats_step_{step_num}.json'
            if stats_file.exists():
                stats_file.unlink()
        
        print(f"[Cleanup] ğŸ—‘ï¸ Cleaned up {len(files_to_delete)} old checkpoints")

    def _log_w2ml_progress(self, step, total_steps):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        
        print(f"\n[W2ML Progress] Step {step}/{total_steps} ({step/total_steps*100:.1f}%)")
        
        if len(self.w2ml_stats['difficulty_scores']) > 0:
            recent_difficulty = self.w2ml_stats['difficulty_scores'][-1]
            print(f"  - Recent difficulty: {recent_difficulty:.6f}")
        
        if len(self.w2ml_stats['detection_rates']) > 0:
            recent_detection = self.w2ml_stats['detection_rates'][-1]
            print(f"  - Recent detection rate: {recent_detection:.1f}%")
        
        if len(self.w2ml_stats['batch_sizes']) > 0:
            recent_batch_size = self.w2ml_stats['batch_sizes'][-1]
            print(f"  - Recent batch size: {recent_batch_size}")
        
        print(f"  - Total learning steps: {self.w2ml_stats['total_learning_steps']}")

    def _final_w2ml_analysis(self):
        """ìµœì¢… W2ML ë¶„ì„"""
        
        print("\n" + "="*80)
        print("FINAL TRUE CONTINUAL W2ML ANALYSIS")
        print("="*80)
        
        total_steps = self.w2ml_stats['total_learning_steps']
        total_hard_negatives = self.w2ml_stats['hard_negative_count']
        total_hard_positives = self.w2ml_stats['hard_positive_count']
        
        if total_steps > 0:
            avg_difficulty = sum(self.w2ml_stats['difficulty_scores']) / len(self.w2ml_stats['difficulty_scores'])
            avg_amplification = sum(self.w2ml_stats['weight_amplifications']) / len(self.w2ml_stats['weight_amplifications'])
            avg_detection_rate = sum(self.w2ml_stats['detection_rates']) / len(self.w2ml_stats['detection_rates'])
            avg_batch_size = sum(self.w2ml_stats['batch_sizes']) / len(self.w2ml_stats['batch_sizes'])
            
            print(f"ğŸ“Š True Continual W2ML Statistics:")
            print(f"   ğŸ”„ Total adaptation steps: {total_steps}")
            print(f"   ğŸ’¡ Average difficulty: {avg_difficulty:.6f}")
            print(f"   âš–ï¸ Average amplification: {avg_amplification:.2f}x")
            print(f"   ğŸ” Average detection rate: {avg_detection_rate:.1f}%")
            print(f"   ğŸ“ Average batch size: {avg_batch_size:.1f}")
            print(f"   ğŸ”´ Hard negatives: {total_hard_negatives}")
            print(f"   ğŸŸ¡ Hard positives: {total_hard_positives}")
            print(f"   ğŸ“ Final dataset position: {self.global_dataset_index}")
            
            # ë°°ì¹˜ í¬ê¸° ë¶„ì„
            target_batch_size = self.config.palm_recognizer.batch_size
            batch_size_achievement = (avg_batch_size / target_batch_size) * 100
            print(f"   ğŸ¯ Batch size achievement: {batch_size_achievement:.1f}%")
            
            # Faiss ìµœì í™” ì„±ê³¼
            if len(self.w2ml_stats['faiss_speedups']) > 0:
                avg_speedup = sum(self.w2ml_stats['faiss_speedups']) / len(self.w2ml_stats['faiss_speedups'])
                max_speedup = max(self.w2ml_stats['faiss_speedups'])
                
                print(f"\nğŸš€ Faiss Optimization Performance:")
                print(f"   âš¡ Average speedup: {avg_speedup:.0f}x")
                print(f"   ğŸ”¥ Maximum speedup: {max_speedup:.0f}x")
                print(f"   ğŸ”§ Optimization status: {'Active' if FAISS_AVAILABLE else 'Fallback mode'}")
            
            # ì„±ëŠ¥ ê°œì„  ë¶„ì„
            if len(self.w2ml_stats['detection_rates']) >= 10:
                early_detection = sum(self.w2ml_stats['detection_rates'][:5]) / 5
                late_detection = sum(self.w2ml_stats['detection_rates'][-5:]) / 5
                improvement = late_detection - early_detection
                
                print(f"\nğŸ“ˆ Learning Progression:")
                print(f"   ğŸŒ… Early detection rate: {early_detection:.1f}%")
                print(f"   ğŸŒ† Late detection rate: {late_detection:.1f}%")
                print(f"   ğŸ“ˆ Detection improvement: {improvement:+.1f}%")
                
                if improvement > 5:
                    print(f"   âœ… W2ML learning is improving!")
                elif improvement > 0:
                    print(f"   ğŸ‘ Stable W2ML performance")
                else:
                    print(f"   ğŸ” Performance may need optimization")
            
            # ìµœì¢… í‰ê°€
            print(f"\nğŸ”¬ True Continual W2ML Implementation:")
            print(f"   ğŸ“– Proper batch size: âœ… {'Achieved' if batch_size_achievement > 95 else 'Needs improvement'}")
            print(f"   ğŸš€ Faiss acceleration: âœ… {'Active' if FAISS_AVAILABLE else 'Fallback mode'}")
            print(f"   ğŸ¯ Hard sample detection: âœ… {'Excellent' if avg_detection_rate > 15 else 'Good' if avg_detection_rate > 10 else 'Moderate'}")
            print(f"   âš–ï¸ W2ML mathematics: âœ… Verified")
            print(f"   ğŸ”„ Checkpoint resume: âœ… Implemented")
            
            if avg_detection_rate > 15 and batch_size_achievement > 95:
                print(f"   ğŸ‰ TRUE CONTINUAL W2ML: FULL SUCCESS!")
            elif avg_detection_rate > 10 and batch_size_achievement > 90:
                print(f"   âœ… TRUE CONTINUAL W2ML: GOOD PERFORMANCE")
            else:
                print(f"   ğŸ”§ TRUE CONTINUAL W2ML: NEEDS FURTHER OPTIMIZATION")
                
        print("="*80)

    def save_system_state(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ ì €ì¥ (ìµœì¢… í˜¸ì¶œìš©)"""
        
        # ğŸ”¥ ì‚¬ìš©ì ì§€ì • ì €ì¥ ê²½ë¡œ
        custom_save_path = Path('/content/drive/MyDrive/CoCoNut_STAR')
        custom_save_path.mkdir(parents=True, exist_ok=True)
        
        # ê¸°ë³¸ ì €ì¥ ê²½ë¡œë„ ìœ ì§€
        storage_path = Path(self.config.replay_buffer.storage_path)
        storage_path.mkdir(parents=True, exist_ok=True)
        
        # ğŸ”¥ ìµœì¢… í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©ì ì§€ì • ê²½ë¡œì— ì €ì¥
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì‚¬ìš©ì ì§€ì • ê²½ë¡œì— ì €ì¥
        custom_learner_path = custom_save_path / f'coconut_w2ml_model_{timestamp}.pth'
        custom_predictor_path = custom_save_path / f'coconut_predictor_model_{timestamp}.pth'
        
        torch.save(self.learner_net.state_dict(), custom_learner_path)
        torch.save(self.predictor_net.state_dict(), custom_predictor_path)
        
        # ê¸°ë³¸ ê²½ë¡œì—ë„ ì €ì¥ (í˜¸í™˜ì„±)
        learner_path = storage_path / 'corrected_w2ml_learner.pth'
        predictor_path = storage_path / 'corrected_w2ml_predictor.pth'
        torch.save(self.learner_net.state_dict(), learner_path)
        torch.save(self.predictor_net.state_dict(), predictor_path)
        
        # ğŸ”¥ ìˆ˜ì •ëœ W2ML í†µê³„ë¥¼ ì‚¬ìš©ì ì§€ì • ê²½ë¡œì— ì €ì¥
        custom_stats_path = custom_save_path / f'coconut_w2ml_stats_{timestamp}.json'
        w2ml_stats_path = storage_path / 'corrected_w2ml_stats.json'
        
        import json
        stats_to_save = {
            'hard_negative_count': self.w2ml_stats['hard_negative_count'],
            'hard_positive_count': self.w2ml_stats['hard_positive_count'],
            'total_learning_steps': self.w2ml_stats['total_learning_steps'],
            'difficulty_scores': self.w2ml_stats['difficulty_scores'],
            'weight_amplifications': self.w2ml_stats['weight_amplifications'],
            'faiss_speedups': self.w2ml_stats['faiss_speedups'],
            'processing_times': self.w2ml_stats['processing_times'],
            'batch_sizes': self.w2ml_stats['batch_sizes'],
            'detection_rates': self.w2ml_stats['detection_rates'],
            # ì„¤ì • ì •ë³´
            'target_batch_size': self.config.palm_recognizer.batch_size,
            'pos_threshold': self.adaptive_contrastive_loss.similarity_threshold_pos,
            'neg_threshold': self.adaptive_contrastive_loss.similarity_threshold_neg,
            'faiss_available': FAISS_AVAILABLE,
            'gpu_available': torch.cuda.is_available(),
            # ì¶”ê°€ ì •ë³´
            'save_timestamp': timestamp,
            'total_adaptation_steps': self.learner_step_count,
            'final_dataset_position': self.global_dataset_index,
            'model_architecture': 'CCNet',
            'w2ml_version': 'CompleteW2MLSupConLoss',
            'continual_learning': True,
            'checkpoint_resume': True
        }
        
        # ì–‘ìª½ ê²½ë¡œì— í†µê³„ ì €ì¥
        with open(custom_stats_path, 'w') as f:
            json.dump(stats_to_save, f, indent=2)
        with open(w2ml_stats_path, 'w') as f:
            json.dump(stats_to_save, f, indent=2)
        
        # ğŸ”¥ ëª¨ë¸ ë¡œë“œ ë°©ë²•ì„ ë‹´ì€ README íŒŒì¼ ìƒì„±
        readme_content = f"""# True Continual CoCoNut W2ML Trained Model

## ëª¨ë¸ ì •ë³´
- ì €ì¥ ì‹œê°„: {timestamp}
- ì´ ì ì‘ ìŠ¤í…: {self.learner_step_count}
- ë°ì´í„°ì…‹ ì™„ë£Œ: {self.global_dataset_index}ê°œ ì²˜ë¦¬
- í•˜ë“œ ìƒ˜í”Œ íƒì§€: {self.w2ml_stats['hard_negative_count'] + self.w2ml_stats['hard_positive_count']}ê°œ
- ì•„í‚¤í…ì²˜: CCNet with True Continual W2ML
- ì²´í¬í¬ì¸íŠ¸ ë³µì›: ì§€ì›ë¨

## íŒŒì¼ ì„¤ëª…
- `coconut_w2ml_model_{timestamp}.pth`: ìµœì¢… í•™ìŠµëœ W2ML ëª¨ë¸ (learner)
- `coconut_predictor_model_{timestamp}.pth`: ì˜ˆì¸¡ìš© ëª¨ë¸ (predictor)
- `coconut_w2ml_stats_{timestamp}.json`: í•™ìŠµ í†µê³„ ë° ì„±ëŠ¥ ì§€í‘œ

## ëª¨ë¸ ë¡œë“œ ë°©ë²•

```python
import torch
from models.ccnet_model import ccnet

# ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±
model = ccnet(num_classes=600, weight=0.8)

# í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
model.load_state_dict(torch.load('coconut_w2ml_model_{timestamp}.pth'))
model.eval()

# íŠ¹ì§• ì¶”ì¶œ ì‚¬ìš© ì˜ˆì‹œ
with torch.no_grad():
    features = model.getFeatureCode(input_image)
```

## ì²´í¬í¬ì¸íŠ¸ ë³µì› ê¸°ëŠ¥
ì´ ëª¨ë¸ì€ True Continual Learningì„ ì§€ì›í•©ë‹ˆë‹¤:
- í•™ìŠµ ì¤‘ë‹¨ ì‹œ ìë™ìœ¼ë¡œ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›
- `/content/drive/MyDrive/CoCoNut_STAR/checkpoints/` í´ë”ì— ì²´í¬í¬ì¸íŠ¸ ì €ì¥
- ì˜µí‹°ë§ˆì´ì € ìƒíƒœ, ë¦¬í”Œë ˆì´ ë²„í¼, ëª¨ë“  í†µê³„ í¬í•¨

## ì„±ëŠ¥ ì •ë³´
- ì´ í•™ìŠµ ìŠ¤í…: {self.w2ml_stats['total_learning_steps']}
- Hard Negative íƒì§€: {self.w2ml_stats['hard_negative_count']}ê°œ
- Hard Positive íƒì§€: {self.w2ml_stats['hard_positive_count']}ê°œ
- Faiss ìµœì í™”: {'ì‚¬ìš©' if FAISS_AVAILABLE else 'ë¯¸ì‚¬ìš©'}
- ì²´í¬í¬ì¸íŠ¸ ìœ„ì¹˜: Step {self.learner_step_count}, Data {self.global_dataset_index}

## ì—°ì† í•™ìŠµ ì¬ê°œ ë°©ë²•
```python
# ìƒˆë¡œìš´ ë°ì´í„°ë¡œ í•™ìŠµ ì¬ê°œ
from framework.coconut import CoconutSystem

system = CoconutSystem(config)  # ìë™ìœ¼ë¡œ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›
system.run_experiment()  # ì¤‘ë‹¨ëœ ì§€ì ë¶€í„° ì´ì–´ì„œ í•™ìŠµ
```

Generated by True Continual CoCoNut W2ML System
Supports checkpoint resume and never loses progress!
"""
        
        readme_path = custom_save_path / f'README_coconut_{timestamp}.md'
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        print(f"[System] âœ… True Continual CoCoNut W2ML ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
        print(f"  ğŸ¯ ì‚¬ìš©ì ì§€ì • ê²½ë¡œ: {custom_save_path}")
        print(f"  ğŸ“ Learner ëª¨ë¸: {custom_learner_path.name}")
        print(f"  ğŸ“ Predictor ëª¨ë¸: {custom_predictor_path.name}")
        print(f"  ğŸ“Š í†µê³„ íŒŒì¼: {custom_stats_path.name}")
        print(f"  ğŸ“– README: {readme_path.name}")
        print(f"  ğŸ• íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")
        print(f"  ğŸ“ˆ ì´ ì ì‘ ìŠ¤í…: {self.learner_step_count}")
        print(f"  ğŸ“ ë°ì´í„°ì…‹ ì™„ë£Œ: {self.global_dataset_index}")
        print(f"\n[System] ğŸ‰ TRUE CONTINUAL COCONUT W2ML completed!")
        print(f"[System] ğŸ¥¥ True continual learning with checkpoint resume!")
        print(f"[System] ğŸ’¾ Models saved to: /content/drive/MyDrive/CoCoNut_STAR")
        print(f"[System] ğŸ”„ Next run will auto-resume from checkpoints!")